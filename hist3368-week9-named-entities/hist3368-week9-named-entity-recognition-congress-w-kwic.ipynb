{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hist 3368 - Week 9 - Named Entity Recognition\n",
    "\n",
    "#### by Jo Guldi, expanding on a prompt from https://www.geeksforgeeks.org/python-named-entity-recognition-ner-using-spacy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPECIAL INSTRUCTIONS FOR THIS SESSION ON M2:**\n",
    "\n",
    "    For \"Memory,\" put \"30GB\"\n",
    "\n",
    "    To the \"custom environment settings\" space, add this: \"source /hpc/applications/python_environments/spacy/bin/activate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll learn how to use the powerful SPACY software library to search for \"named entities.\" \n",
    "\n",
    "A ***named entity*** can be a person's name, the name of a country, the date of a famous event, a kind of money, a corporation, or practically anything else. \n",
    "\n",
    " * In the case of this notebook, we'll learn about some of the results of named entity recognition.  We'll see how SPACY categorized named entities as people, places, dates, etc.  \n",
    "\n",
    " * We'll then apply SPACY to the Dallas City Council minutes, where we'll see that named entity recognition is capable of detecting the names of local government regulations in Texas\n",
    " \n",
    " We will notice that the results of grammatical analysis look *very* different from the results of using WordNet.  The controlled vocabulary organized by linguists at Princeton, for example, doesn't know anything about local government regulations in Dallas, Texas.  But by using grammatical analysis, Spacy will pick up on the names of common government regulations specific to Dallas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Code to Extract Named Entities\n",
    "\n",
    "We'll learn about the *Spacy* software package.  To work with Spacy, you need to tell M2 to begin a session with special parameters that include loading the background software for Spacy.\n",
    "\n",
    "* Please note that to use spaCy on M2 you must go to My Interactive Sessions/JupyterLab and add **source** /hpc/applications/python_environments/spacy/bin/activate to the “Custom environment settings” field.\n",
    "\n",
    "We'll learn about the\n",
    "\n",
    "    nlp(string)\n",
    "    \n",
    "command from Spacy, which tells spacy to extract all named entities from a string of text.   \n",
    "\n",
    "We'll also learn how to read the output of the nlp() command, which creates a dataset whose contents are typically called with a for loop requesting each of the following:\n",
    "\n",
    "    *.ents -- that is, each of the entities for the document created\n",
    "    *.ent_label_ -- for each entity, there is an ent_label that includes information about what kind of named entity Spacy has found.  \n",
    "\n",
    "#### Counting Named Entities and Thinking About Counting\n",
    "\n",
    "We will also see some familiar commands for grouping and counting, which will allow us to model the number of events, persons, corporations, and nationalities mentioned over time with NER.\n",
    "\n",
    "\t.explode().dropna()\n",
    "\t.value_counts()\n",
    "\t.nlargest()\n",
    "\t.groupby()\n",
    "\t.count()\n",
    "\t.unique()\n",
    "\n",
    "We’ll also talk about the best way of counting over time.  Consider the following propositions.  \n",
    "\n",
    "If I want to understand the most important places discussed over time in the Dallas City Council debates, should I pay most attention to:\n",
    "\n",
    "1. the neighborhoods referenced the highest number of months?\n",
    "2. the neighborhoods that are referenced with the maximum number of times in any single month?\n",
    "3. the neighborhoods that are referenced the most frequently overall? \n",
    "\n",
    "We will give you the code for each of these measurements.\n",
    "\n",
    "In theory, each of these measurements might contribute to some analysis — the question is how each of them produces a different model of “significance.” \n",
    "\n",
    "For instance, the first measure — (1) neighborhoods referenced over the most months — will give you a list of neighborhoods referenced consistently, whereas the second measure — (2) the highest relative peak — will give you neighborhoods that became suddenly important at one moment of time.  Both measures might be potentially useful to know about.  \n",
    "\n",
    "We will  be asking you to think about how changing what is being measured changes the meaning of the analysis. \n",
    "\n",
    "#### Learn How to Write a Function\n",
    "\n",
    "We'll learn how to define a function, using the command\n",
    "\n",
    "    def \n",
    "\n",
    "and how to return information from a function with the command\n",
    "\n",
    "    return()\n",
    "\n",
    "We’ll define a function called ***ner_finder(sentence, Label1)*** -- which will return all the named entities of kind label1 from a string called sentence.  \n",
    "\n",
    "In theory, ner_finder() can be applied to the text column of any dataset and produce answers, although it is very slow, as is the way with grammatical analysis. The simple formula for applying ner_finder to a column of text, looking only for the items labeled 'LAW', is this:\n",
    "\n",
    "    results = [ner_finder(sentence, 'LAW') for sentence in dallas_minutes['Text']]\n",
    "\n",
    "#### Learn Some More About Speed\n",
    "\n",
    "Finally, this lesson adds a few tools that are useful for speeding up slow code and for estimating how long a coding process will take. These tools, while not essential to the work of this notebook, may be useful as you try to implement code on large-scale datasets.\n",
    "\n",
    "We’ll learn about speeding up code using “parallel” processing, which is often faster for large datasets than list comprehension.  We’ll learn the function\n",
    "\n",
    "\t.apply()\n",
    "\n",
    "which \"applies\" a task over over each item in a list or column -- much like a for-loop. \n",
    "\n",
    "To use .apply() with the function ner_finder(), we’ll use the grammar\n",
    "\n",
    "\t.apply(lambda x: [function to be applied])\n",
    "\n",
    "to apply a function like ner_finder() to each item in a pandas column.\n",
    "\n",
    "\n",
    "\n",
    "Technically, you don't need to understand .apply() to work with NER.  We'll learn that these two lines of code do *exactly the same thing,* although one may be faster:\n",
    "\n",
    "    [ner_finder(sentence, 'LAW') for sentence in dallas_minutes['Text']] -- this is the 'list comprehension' method explained above\n",
    "\tdallas_minutes['Text'].apply(lambda x: ner_finder(x, 'LAW')) -- this is the .apply() method \n",
    "\n",
    "\n",
    "We’ll learn a little about timing data processes with the **time** package.  We’ll use the command\n",
    "\n",
    "\ttime.time() \n",
    "\n",
    "To take the time in milliseconds. We'll use this timer to compare the speed of different approaches to the same code. \n",
    "\n",
    "This tool will allow you to decide, for yourself, between two comparable coding approaches, choosing the one that is more efficient.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, spacy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'lemmatizer', 'tagger'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting an error? \n",
    "\n",
    "* Please note that to use spaCy on M2 you must go to My Interactive Sessions/JupyterLab and add **source /hpc/applications/python_environments/spacy/bin/activate** to the **“Custom environment settings”** field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load the speeches of Congress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shared/history\n"
     ]
    }
   ],
   "source": [
    "cd /shared/history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = pd.read_csv(\"congress1967-2010.csv\")\n",
    "#congress = pd.read_csv(\"eighties_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/digital-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_1968 = congress[congress['year'] == 1968]\n",
    "congress_1968[\"date\"] = pd.to_datetime(congress_1968[\"date\"])\n",
    "congress_1968[\"month_year\"] = pd.to_datetime(congress_1968[\"month_year\"])\n",
    "congress_1968[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package SPACY uses the grammar of sentences to make an educated \"guess\" about what kind of entity is under consideration. \n",
    "\n",
    "Note that the package *isn't* applying a controlled vocabulary of all possible dates or years. It's just looking at grammar -- using clues like the part-of-speech of each of the words and what prepositions are nearby to make a guess about whether each entity is the name of a law, a place, or a person.\n",
    "\n",
    "It's actually *generating* a new controlled vocabulary on the basis of grammar.\n",
    "\n",
    "Named entity recognition can be used as an alternative to controlled vocabulary to create a much more specific, tailor-made vocabulary appropriate to a textbase.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The United Kingdom passed the Representation of the People Act in 1928, giving women in Wales and London, among others, the right to vote.\"\n",
    "  \n",
    "doc = nlp(sentence)\n",
    "  \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the program has correctly identified a \"geo political entity,\" or GPE, in \"The United Kingdom.\"  It has correctly guessed that we're talking about a law, although it guesses that the name of the law is \"the People Act\" rather than \"The Representation of the People Act.\"  It notes that the correct year is important, 1928.  \n",
    "\n",
    "It's important for you to notice that *some of the information is correct* and *some of the information is rather bad*.  As always, you'll need to apply your own best judgment to the situation.\n",
    "\n",
    "Other things you might be curious about:\n",
    "\n",
    " * In the program in question, we're parsing through a list of information called *.ents*, which spacy creates when applying the command **nlp()** to a document. Our document is called called *doc*, so the entities list is called *.ents*.  \n",
    "\n",
    " * .ents contains information coded by Spacy as the \".text,\" or the word that appears to have a significance, the starting character number, the ending character number, and a .label_, which corresponds to a grammatical category such as \"date,\" \"law,\" \"geo-political entity,\" etc.\n",
    "\n",
    " * Note: This information is hidden in Spacy's own \"Doc\" data type.  You don't really need to know anything for our purposes beyond how to call information from a Spacy doc by asking for ent.label_ etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more complicated sentence. This one mentions nations and people as well as languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"In 1066, the Norman leader William the Conquerer -- who came from the north of France -- invaded England at the Battle of Hastings, and his success is the reason why the English language has so many French words in it.\"\n",
    "  \n",
    "doc = nlp(sentence2)\n",
    "  \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time we have many more named entities.  Some are classified \"DATE\" -- like 1066. Others are classified \"EVENT\" -- like the Battle of Hastings.  We also have a person, William the Conquerer, a LANGUAGE -- English, two nationalities or regional identifications for people (NORP), and two geo-political lentities (French, English).\n",
    "\n",
    "One could imagine creating a digital history project that investigated any of these categories. For instance, if we were interested in how Congress handled immigration over the twentieth century, it might be interesting to count references to different nationalities and languages.  If we were interested in how Reddit talked about climate change, it might be interesting to track the people and place-names that came up in discussions of environmental contamination.  \n",
    "\n",
    "However, in this assignment we're going to ask how parliament talked about events and dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justevents = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'EVENT':\n",
    "        justevents.append(ent.text)\n",
    "        \n",
    "justevents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply NER to Congress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our event recognizer to just a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_1968 = congress_1968.reset_index()\n",
    "congress_1968['speech'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a spacy parse on the speeches from one year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_1968_text = ' '.join([speech for speech in congress_1968['speech']][:500]) # glue together the first 500 speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(congress_1968_text) # perform NLP \n",
    "  \n",
    "for ent in doc.ents[:10]: # for each entity found, print the name of the entity, character info, and its category.\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for just the dates referenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundlaws = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'WORK_OF_ART':\n",
    "        foundlaws.append(ent.text)\n",
    "        \n",
    "foundlaws[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the number in square brackets to see the entire list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundpeople = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        foundpeople.append(ent.text)\n",
    "foundpeople[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundevents = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'EVENT':\n",
    "        foundevents.append(ent.text)\n",
    "        \n",
    "foundevents[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a new function.  Functions are defined using the command\n",
    "\n",
    "    def FUNCTIONNAME(object1, object2):\n",
    "    \n",
    "We'll call our new function ner_finder.\n",
    "\n",
    "It will take two commands: sentence, and label1.\n",
    "\n",
    "It will return any matches for a sentence and a label. To tell Python what to return to the user after running a function, we use \n",
    "\n",
    "    return()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_finder(sentence, label1):\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    foundstuff = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ ==  label1:\n",
    "            foundstuff.append(ent.text)\n",
    "    \n",
    "    return(foundstuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out our new function on some of the categories that Spacy can detect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopolitical = ner_finder(congress_1968_text, 'GPE')\n",
    "geopolitical[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laws = ner_finder(congress_1968_text, 'LAW')\n",
    "laws[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = ner_finder(congress_1968_text, 'PERSON')\n",
    "people[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = ner_finder(congress_1968_text, 'LOC')\n",
    "places[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ner_finder(congress_1968_text, 'DATE')\n",
    "dates[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations = ner_finder(congress_1968_text, 'ORG')\n",
    "organizations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = ner_finder(congress_1968_text, 'EVENT')\n",
    "events[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the spacy *nlp* command to everything in the 'Text' column. The formula that we used above was \n",
    "\n",
    "    nlp(sentence)\n",
    "\n",
    "This time, however, we want to apply *nlp()* to every row in a pandas dataframe.   Here's one way of writing such a command:\n",
    "\n",
    "    dallas_minutes['NLP'] = [ner_finder(sentence, 'LAW') for sentence in dallas_minutes['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [ner_finder(speech, 'LAW') for speech in congress_1968['speech'][:20]]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "You'll see a series of blank entries -- [] -- which represent rows in the database, or congressional speeches, where no mentions of concrete laws were detected.  You'll also see a few references to the Constitution.  These correspond to the speeches in our list of Congressional speeches from the year 1968.  We can use the order of the speeches to work with extra information about the speeches' date of delivery, the speaker who gave them, and other important facts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's run the same command again, but this time save it as a new column called 'law,' so that we can see the speaker, date of delivery of the speech, and other information.\n",
    "Because the code is very slow, first we'll downsample, creating the variable *congress_1968_small.*  \n",
    "\n",
    "If you want to run the code on all the debates from 1968, change *congress_1968_small* below to *congress_1968*.  Give it at least an hour to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_1968_small = congress_1968.sample(1000) # downsamples the full dataset to a smaller dataset of n random rows\n",
    "\n",
    "congress_1968_ner = congress_1968_small.copy()\n",
    "congress_1968_ner['law'] = [ner_finder(speech, 'ORG') for speech in congress_1968_small['speech']] # change congress_1968_small to congress_1968 to run all the data\n",
    "congress_1968_ner[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the name of getting you more results quickly, let's load some pre-computed named entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$HOME/shared/history\n"
     ]
    }
   ],
   "source": [
    "cd $HOME/shared/history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files we want are labeled *stanford_congressional_records_named_entities_???.csv* where *???* can be replaced with any possible named entity tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "named_entities1 = pd.read_csv(\"stanford_congressional_records_named_entities_LAW.csv\", error_bad_lines=False, warn_bad_lines=False,\n",
    "                                  quoting=csv.QUOTE_NONE, encoding='utf-8', engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>speech_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>file</th>\n",
       "      <th>parsed_text</th>\n",
       "      <th>clean_NE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1820115</th>\n",
       "      <td>2</td>\n",
       "      <td>430000001</td>\n",
       "      <td>Bainbridge Wadleigh</td>\n",
       "      <td>18730304</td>\n",
       "      <td>The VICE-PRESIDENT</td>\n",
       "      <td>03041873.txt</td>\n",
       "      <td>Bainbridge Wadleigh</td>\n",
       "      <td>Bainbridge Wadleigh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820137</th>\n",
       "      <td>98</td>\n",
       "      <td>430000004</td>\n",
       "      <td>Those assembled in the Senate Chamber proceede...</td>\n",
       "      <td>18730304</td>\n",
       "      <td>The VICE-PRESIDENT</td>\n",
       "      <td>03041873.txt</td>\n",
       "      <td>Those assembled in the Senate Chamber proceede...</td>\n",
       "      <td>the Senate Chamber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820138</th>\n",
       "      <td>100</td>\n",
       "      <td>430000004</td>\n",
       "      <td>The Supreme Court of the United States</td>\n",
       "      <td>18730304</td>\n",
       "      <td>The VICE-PRESIDENT</td>\n",
       "      <td>03041873.txt</td>\n",
       "      <td>The Supreme Court of the United States</td>\n",
       "      <td>The Supreme Court</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820139</th>\n",
       "      <td>101</td>\n",
       "      <td>430000004</td>\n",
       "      <td>The SergeantatArms of the Senate</td>\n",
       "      <td>18730304</td>\n",
       "      <td>The VICE-PRESIDENT</td>\n",
       "      <td>03041873.txt</td>\n",
       "      <td>The SergeantatArms of the Senate</td>\n",
       "      <td>Senate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820140</th>\n",
       "      <td>102</td>\n",
       "      <td>430000004</td>\n",
       "      <td>The Committee of Arrangements</td>\n",
       "      <td>18730304</td>\n",
       "      <td>The VICE-PRESIDENT</td>\n",
       "      <td>03041873.txt</td>\n",
       "      <td>The Committee of Arrangements</td>\n",
       "      <td>The Committee of Arrangements</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X1  speech_id                                           sentence  \\\n",
       "1820115    2  430000001                                Bainbridge Wadleigh   \n",
       "1820137   98  430000004  Those assembled in the Senate Chamber proceede...   \n",
       "1820138  100  430000004             The Supreme Court of the United States   \n",
       "1820139  101  430000004                   The SergeantatArms of the Senate   \n",
       "1820140  102  430000004                      The Committee of Arrangements   \n",
       "\n",
       "             date             speaker          file  \\\n",
       "1820115  18730304  The VICE-PRESIDENT  03041873.txt   \n",
       "1820137  18730304  The VICE-PRESIDENT  03041873.txt   \n",
       "1820138  18730304  The VICE-PRESIDENT  03041873.txt   \n",
       "1820139  18730304  The VICE-PRESIDENT  03041873.txt   \n",
       "1820140  18730304  The VICE-PRESIDENT  03041873.txt   \n",
       "\n",
       "                                               parsed_text  \\\n",
       "1820115                                Bainbridge Wadleigh   \n",
       "1820137  Those assembled in the Senate Chamber proceede...   \n",
       "1820138             The Supreme Court of the United States   \n",
       "1820139                   The SergeantatArms of the Senate   \n",
       "1820140                      The Committee of Arrangements   \n",
       "\n",
       "                              clean_NE  \n",
       "1820115            Bainbridge Wadleigh  \n",
       "1820137             the Senate Chamber  \n",
       "1820138              The Supreme Court  \n",
       "1820139                         Senate  \n",
       "1820140  The Committee of Arrangements  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities1.sort_values(by = 'date')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>speech_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>file</th>\n",
       "      <th>parsed_text</th>\n",
       "      <th>clean_NE</th>\n",
       "      <th>law</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>month_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1820115</th>\n",
       "      <td>2</td>\n",
       "      <td>430000001</td>\n",
       "      <td>Bainbridge Wadleigh</td>\n",
       "      <td>1873-03-04</td>\n",
       "      <td>The VICE-PRESIDENT</td>\n",
       "      <td>03041873.txt</td>\n",
       "      <td>Bainbridge Wadleigh</td>\n",
       "      <td>Bainbridge Wadleigh</td>\n",
       "      <td>Bainbridge Wadleigh</td>\n",
       "      <td>3</td>\n",
       "      <td>1873</td>\n",
       "      <td>1873-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820137</th>\n",
       "      <td>98</td>\n",
       "      <td>430000004</td>\n",
       "      <td>Those assembled in the Senate Chamber proceede...</td>\n",
       "      <td>1873-03-04</td>\n",
       "      <td>The VICE-PRESIDENT</td>\n",
       "      <td>03041873.txt</td>\n",
       "      <td>Those assembled in the Senate Chamber proceede...</td>\n",
       "      <td>the Senate Chamber</td>\n",
       "      <td>the Senate Chamber</td>\n",
       "      <td>3</td>\n",
       "      <td>1873</td>\n",
       "      <td>1873-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820138</th>\n",
       "      <td>100</td>\n",
       "      <td>430000004</td>\n",
       "      <td>The Supreme Court of the United States</td>\n",
       "      <td>1873-03-04</td>\n",
       "      <td>The VICE-PRESIDENT</td>\n",
       "      <td>03041873.txt</td>\n",
       "      <td>The Supreme Court of the United States</td>\n",
       "      <td>The Supreme Court</td>\n",
       "      <td>The Supreme Court</td>\n",
       "      <td>3</td>\n",
       "      <td>1873</td>\n",
       "      <td>1873-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820139</th>\n",
       "      <td>101</td>\n",
       "      <td>430000004</td>\n",
       "      <td>The SergeantatArms of the Senate</td>\n",
       "      <td>1873-03-04</td>\n",
       "      <td>The VICE-PRESIDENT</td>\n",
       "      <td>03041873.txt</td>\n",
       "      <td>The SergeantatArms of the Senate</td>\n",
       "      <td>Senate</td>\n",
       "      <td>Senate</td>\n",
       "      <td>3</td>\n",
       "      <td>1873</td>\n",
       "      <td>1873-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820140</th>\n",
       "      <td>102</td>\n",
       "      <td>430000004</td>\n",
       "      <td>The Committee of Arrangements</td>\n",
       "      <td>1873-03-04</td>\n",
       "      <td>The VICE-PRESIDENT</td>\n",
       "      <td>03041873.txt</td>\n",
       "      <td>The Committee of Arrangements</td>\n",
       "      <td>The Committee of Arrangements</td>\n",
       "      <td>The Committee of Arrangements</td>\n",
       "      <td>3</td>\n",
       "      <td>1873</td>\n",
       "      <td>1873-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321980</th>\n",
       "      <td>5202</td>\n",
       "      <td>1110179027</td>\n",
       "      <td>I ask unanimous consent that when the Senate c...</td>\n",
       "      <td>2010-12-22</td>\n",
       "      <td>Mr. BAYH</td>\n",
       "      <td>12222010.txt</td>\n",
       "      <td>I ask unanimous consent that when the Senate c...</td>\n",
       "      <td>Senate</td>\n",
       "      <td>Senate</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321981</th>\n",
       "      <td>5210</td>\n",
       "      <td>1110179029</td>\n",
       "      <td>I further ask unanimous consent that when the ...</td>\n",
       "      <td>2010-12-22</td>\n",
       "      <td>Mr. BAYH</td>\n",
       "      <td>12222010.txt</td>\n",
       "      <td>I further ask unanimous consent that when the ...</td>\n",
       "      <td>Senate</td>\n",
       "      <td>Senate</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321982</th>\n",
       "      <td>5216</td>\n",
       "      <td>1110179029</td>\n",
       "      <td>the Journal of proceedings be approved to date</td>\n",
       "      <td>2010-12-22</td>\n",
       "      <td>Mr. BAYH</td>\n",
       "      <td>12222010.txt</td>\n",
       "      <td>the Journal of proceedings be approved to date</td>\n",
       "      <td>Journal</td>\n",
       "      <td>Journal</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321954</th>\n",
       "      <td>5097</td>\n",
       "      <td>1110179025</td>\n",
       "      <td>Senate Republicans tactics reached a new low a...</td>\n",
       "      <td>2010-12-22</td>\n",
       "      <td>Mr. LEARY</td>\n",
       "      <td>12222010.txt</td>\n",
       "      <td>Senate Republicans tactics reached a new low a...</td>\n",
       "      <td>Senate</td>\n",
       "      <td>Senate</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983965</th>\n",
       "      <td>7508</td>\n",
       "      <td>1110178801</td>\n",
       "      <td>I am disappointed that the Senate could not ag...</td>\n",
       "      <td>2010-12-22</td>\n",
       "      <td>Mr. GRASSLEY</td>\n",
       "      <td>12222010.txt</td>\n",
       "      <td>I am disappointed that the Senate could not ag...</td>\n",
       "      <td>Senate</td>\n",
       "      <td>Senate</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22679911 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X1   speech_id                                           sentence  \\\n",
       "1820115      2   430000001                                Bainbridge Wadleigh   \n",
       "1820137     98   430000004  Those assembled in the Senate Chamber proceede...   \n",
       "1820138    100   430000004             The Supreme Court of the United States   \n",
       "1820139    101   430000004                   The SergeantatArms of the Senate   \n",
       "1820140    102   430000004                      The Committee of Arrangements   \n",
       "...        ...         ...                                                ...   \n",
       "5321980   5202  1110179027  I ask unanimous consent that when the Senate c...   \n",
       "5321981   5210  1110179029  I further ask unanimous consent that when the ...   \n",
       "5321982   5216  1110179029     the Journal of proceedings be approved to date   \n",
       "5321954   5097  1110179025  Senate Republicans tactics reached a new low a...   \n",
       "19983965  7508  1110178801  I am disappointed that the Senate could not ag...   \n",
       "\n",
       "               date             speaker          file  \\\n",
       "1820115  1873-03-04  The VICE-PRESIDENT  03041873.txt   \n",
       "1820137  1873-03-04  The VICE-PRESIDENT  03041873.txt   \n",
       "1820138  1873-03-04  The VICE-PRESIDENT  03041873.txt   \n",
       "1820139  1873-03-04  The VICE-PRESIDENT  03041873.txt   \n",
       "1820140  1873-03-04  The VICE-PRESIDENT  03041873.txt   \n",
       "...             ...                 ...           ...   \n",
       "5321980  2010-12-22            Mr. BAYH  12222010.txt   \n",
       "5321981  2010-12-22            Mr. BAYH  12222010.txt   \n",
       "5321982  2010-12-22            Mr. BAYH  12222010.txt   \n",
       "5321954  2010-12-22           Mr. LEARY  12222010.txt   \n",
       "19983965 2010-12-22        Mr. GRASSLEY  12222010.txt   \n",
       "\n",
       "                                                parsed_text  \\\n",
       "1820115                                 Bainbridge Wadleigh   \n",
       "1820137   Those assembled in the Senate Chamber proceede...   \n",
       "1820138              The Supreme Court of the United States   \n",
       "1820139                    The SergeantatArms of the Senate   \n",
       "1820140                       The Committee of Arrangements   \n",
       "...                                                     ...   \n",
       "5321980   I ask unanimous consent that when the Senate c...   \n",
       "5321981   I further ask unanimous consent that when the ...   \n",
       "5321982      the Journal of proceedings be approved to date   \n",
       "5321954   Senate Republicans tactics reached a new low a...   \n",
       "19983965  I am disappointed that the Senate could not ag...   \n",
       "\n",
       "                               clean_NE                            law  month  \\\n",
       "1820115             Bainbridge Wadleigh            Bainbridge Wadleigh      3   \n",
       "1820137              the Senate Chamber             the Senate Chamber      3   \n",
       "1820138               The Supreme Court              The Supreme Court      3   \n",
       "1820139                          Senate                         Senate      3   \n",
       "1820140   The Committee of Arrangements  The Committee of Arrangements      3   \n",
       "...                                 ...                            ...    ...   \n",
       "5321980                          Senate                         Senate     12   \n",
       "5321981                          Senate                         Senate     12   \n",
       "5321982                         Journal                        Journal     12   \n",
       "5321954                          Senate                         Senate     12   \n",
       "19983965                         Senate                         Senate     12   \n",
       "\n",
       "          year month_year  \n",
       "1820115   1873    1873-03  \n",
       "1820137   1873    1873-03  \n",
       "1820138   1873    1873-03  \n",
       "1820139   1873    1873-03  \n",
       "1820140   1873    1873-03  \n",
       "...        ...        ...  \n",
       "5321980   2010    2010-12  \n",
       "5321981   2010    2010-12  \n",
       "5321982   2010    2010-12  \n",
       "5321954   2010    2010-12  \n",
       "19983965  2010    2010-12  \n",
       "\n",
       "[22679911 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_1968_ner = named_entities1.sort_values(by = 'date').copy()\n",
    "\n",
    "congress_1968_ner['law'] = named_entities1['clean_NE']\n",
    "congress_1968_ner[\"date\"] = pd.to_datetime(congress_1968_ner[\"date\"], format='%Y%m%d')\n",
    "\n",
    "congress_1968_ner[\"month\"] = congress_1968_ner[\"date\"].dt.month\n",
    "congress_1968_ner[\"year\"] = congress_1968_ner[\"date\"].dt.year\n",
    "congress_1968_ner[\"month_year\"] = pd.to_datetime(congress_1968_ner[\"date\"]).dt.to_period('M')\n",
    "\n",
    "congress_1968_ner = congress_1968_ner[congress_1968_ner['year'] == 1968] ### <-- delete this line to work with all available data\n",
    "\n",
    "congress_1968_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also notice that the code hangs for a moment.  *Give the computer a while to do its work.*\n",
    "\n",
    "  * Spacy is an intensive software package, and applying nlp() to many documents can be slow going. Remember how a similarly linguistically-intense command, wn.morphy(), slowed us down elsewhere.\n",
    "  * If you'd like to think about speeding up your code, please consult the \"Tutorial on Speed\" in the Github.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's filter for JUST the NLP findings that are events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the laws most frequently mentioned overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the laws in the law column, we should also notice that the 'law' column we've just made is filled with list.  If multiple laws are referenced within the same speech, the 'law' column lists multiple laws, strung together as a list inside square brackets, whose members are separated by commas:\n",
    "\n",
    "    [the Constitution, Constitution, the Internal Revenue Code]\n",
    "\n",
    "If we want to count like mentions of laws, we'll need to split up this data so that each item in the list gets its own row.  We will follow a basic procedure to work with this data:\n",
    "\n",
    "* In order to count items in a list, we *explode* the list so that each list item gets its own row.  \n",
    "* Then we can use *.value_counts()* to count how many times each appears.\n",
    "* We can use *.nlargest()* to get the most frequent items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most frequently mentioned laws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the laws listed in the 'Laws' column is organized as a *list* -- they're inside square brackets, seaprated by commas. \n",
    "\n",
    "In order to count items in a list, we *explode* the list so that each list item gets its own row.  \n",
    "\n",
    "Then we can use *.value_counts()* to count how many times each appears.\n",
    "\n",
    "We can use *.nlargest()* to get the most frequent items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use 'explode,' 'groupby,' and 'count' to find out how many events were referenced per year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use .explode() to reformat the data so that each entity gets its own row\n",
    "\n",
    "We can use .explode() and .groupby() to tell pandas how to organize the data before using .count() to tell us how many laws are mentioned in each grouping.\n",
    "\n",
    "   * Let's \"explode\" the data so that every law gets its own row.\n",
    "   * Let's group by Month-Year and Laws so that we bundle under each month-year each mention of a law mentioned in that time period.\n",
    "   * We'll use square brackets around 'Laws' once more to tell pandas that we're interested in just working with the Laws column.\n",
    "   * We'll apply .count() to count how many laws are mentioned per Month-Year. This is what we really want to know.\n",
    "\n",
    "First, 'explode' the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_1968_law_exploded = congress_1968_ner.explode('law').dropna()\n",
    "congress_1968_law_exploded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Use *.groupby()* and *.count()* to Count How Many Time Each Law is Mentioned Overall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code \"groups\" the data by the law referenced in the \"law\" column, then \"counts\" how many times each law was mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_counts = congress_1968_law_exploded.groupby(['law'])['law'].count() # count how many laws there are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minor reformatting gives us the answers we want with nicely labeled columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_counts = pd.DataFrame(law_counts) # convert from Series to DataFrame\n",
    "law_counts.columns = ['count'] # label the one column \"count\"\n",
    "law_counts = law_counts.reset_index().sort_values('count', ascending = False) # turn the multi-index into columns 'year' and 'vocab'\n",
    "law_counts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab just the top laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_laws = law_counts[:20]\n",
    "top_laws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Use *.groupby() and *.count()* to Count Laws Mentioned by Month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's \"group\" by month and law and \"count\" how many times each law is mentioned in each month.  NOTE: YOU CAN CHANGE MONTH_YEAR to YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_per_month_counts = congress_1968_law_exploded.groupby(['year', 'law']).size().reset_index(name='count') # count how many unique months correspond to each law\n",
    "law_per_month_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the laws talked about each month *most frequently*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_laws_per_month = law_per_month_counts.groupby(['year']).apply(lambda x: x.sort_values('count', ascending = False)).reset_index(drop=True)\n",
    "top_laws_per_month = top_laws_per_month.groupby('year').head(3)\n",
    "top_laws_per_month[:20]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for the Most Important Laws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways of going about the next step, but we'll show one.  Let's connect this data about the most talked-about laws to our data about dates, in the dataframe laws_per_month.\n",
    "\n",
    "Notice the use of .index, .isin(), and [] to filter for a certain condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the top laws by the maximum count per month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_laws_with_month = law_per_month_counts[law_per_month_counts['law'].isin(top_laws['law'])].sort_values('count', ascending = False) # find top laws over time\n",
    "top_laws_with_month[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort them so that they're in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_laws_with_month = top_laws_with_month.sort_index(inplace=False)\n",
    "top_laws_with_month[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting a Graph of Named Entities Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_laws_with_month['month_year'] = top_laws_with_month['month_year'].astype(str)\n",
    "#top_laws_with_month['month_year'] = pd.to_datetime(top_laws_with_month['month_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "figure(figsize=(8, 6), dpi=300)\n",
    "\n",
    "plt.style.use('seaborn-darkgrid') # this gives us a grid with a dark background.  you can play with this to change the style.\n",
    "palette = plt.get_cmap('tab20b') # this tells matplotlib what colors to use.  you can play with this to change the colors.\n",
    "num=0\n",
    "\n",
    "\n",
    "for entity in set(top_laws_with_month['law']): # one loop for each color/line\n",
    "        \n",
    "        # get points\n",
    "        num+=1 # for each new word, the counter 'num' changes \n",
    "        x = top_laws_with_month.loc[top_laws_with_month['law'] == entity, 'year'] # x points\n",
    "        y = top_laws_with_month.loc[top_laws_with_month['law'] == entity, 'count'] # y points\n",
    "        \n",
    "        #x2, y2 = zip(*sorted(zip(x, y))) # get everything in the right order\n",
    "        \n",
    "        # make a line\n",
    "        plt.plot(x, # x axis \n",
    "             y,  # y axis\n",
    "             '-o', # make dots with lines\n",
    "             color=palette(num), \n",
    "             alpha=0.7,\n",
    "             label = entity) # num tells the plot to choose a different color this time\n",
    "        \n",
    "        # make labels\n",
    "        y3 = max(y) # label lines at their highest point\n",
    "        entity_points = top_laws_with_month[top_laws_with_month['law'] == entity]\n",
    "        x3 = random.choice(entity_points[entity_points['count'] == max(entity_points['count'])]['year'].tolist()) # more for finding the highest point\n",
    "        plt.text(x3, y3, entity, color = palette(num), size = 10) # this is the code to supply a label for each line\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5)) # move the legend\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Figure 1: Top Laws Mentioned in Congress in 1968\", loc='left', fontsize=20, fontweight=5, color='Blue')\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the purposes of clarifying what the graph is showing, I've used \"plt.text()\" to create a label for each line, and assigned that label the same color as each line.  Notice that I've also used the argument \"bbox_to_anchor\" for the legend to move the legend to the left of the chart.\n",
    "\n",
    "*The output is far from ideal.*  I would much prefer to see a graph where the labels aren't overlapping (there is a module for this called adjust_text, which we'll explore later)\n",
    "\n",
    "But the point of this exercise is to show one possible strategy for making charts clearer and more intuitive.\n",
    "\n",
    "It will be the responsibility of each student group to decide on the appropriate strategy for each data set.  An appropriate strategy is one that shows you pertinent information and which is legible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NER for historical research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for word1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about how to dive deeper into research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the word or phrase that you want to look for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = \"the Constitution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned that we can use **str.count()** to count the individual occurrences of a word per speech. str.count() takes one object, the word you're looking for. We're looking for *word1*, which is a variable containing the word 'malpractice.'\n",
    "\n",
    "We can also use **.sort_values()** to tell pandas to sort the data by the values in one column.  .sort_values() takes the arguments \"by\" (where you tell it which column to use, using quotation marks, and \"ascending,\" which can be True or False, depending on how you want your values arranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_1968['keywordcount'] = congress_1968['speech'].str.count(word1)\n",
    "congress_1968.sort_values(by = 'keywordcount', ascending = False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times was word1 used in each time period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_1968['speech'].str.count(word1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_1968 = congress_1968[congress_1968['month'] == 5]\n",
    "august_1968 = congress_1968[congress_1968['month'] == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_1968['speech'].str.count(word1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "august_1968['speech'].str.count(word1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the difference mean?  Read for a little context:\n",
    "\n",
    " [https://en.wikipedia.org/wiki/May_1968](https://en.wikipedia.org/wiki/May_1968)\n",
    " \n",
    " [https://en.wikipedia.org/wiki/1968_Democratic_National_Convention](https://en.wikipedia.org/wiki/1968_Democratic_National_Convention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the context for word1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the context for word1, you must call up the full text for the speeches where the keyword appears.\n",
    "\n",
    "Next, we can count the words that commonly appear around word1.\n",
    "\n",
    "Using .str.count(word1) and .nlargest(n) give you a list of the rownumbers for the n speeches where word1 appears the most frequently.  The rownumbers are stored in the .index for the resulting series. We save the results as the variables *word1_speechnumbers*\n",
    "\n",
    "We can use .loc() and the rownumbers to call up the full text of the speeches that correspond to word1_speechnumbers.  We will save the results as *word1_context.*\n",
    "\n",
    "We then clean the data using stopwording, lowercasing, and punctuation stripping.\n",
    "\n",
    "We then count the words that result using commands for breaking the string into words, giving each word a separate row, dropping empty rows, and counting the results:\n",
    "\n",
    "    str.split().explode().dropna().value_counts()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = spacy.load('en_core_web_sm')\n",
    "stop = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_regex = r'\\b(?:{})\\b'.format('|'.join(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rownumbers of the n speeches that use word1 the most\n",
    "word1_may_speechnumbers = may_1968['speech'].str.count(word1).nlargest(10) \n",
    "word1_august_speechnumbers = august_1968['speech'].str.count(word1).nlargest(10) \n",
    "\n",
    "# get the speeches that mentioned word1 the most\n",
    "word1_may_context = may_1968.loc[list(word1_may_speechnumbers.index)]\n",
    "word1_august_context = august_1968.loc[list(word1_august_speechnumbers.index)]\n",
    "                                \n",
    "# clean up the data\n",
    "word1_may_context['cleanspeech'] = word1_may_context['speech'].str.replace('[^\\w\\s]','').str.lower() # remove punctuation, lowercase\n",
    "word1_may_context['stopworded'] = word1_may_context['cleanspeech'].str.replace(stopwords_regex, '') #stopwording\n",
    "\n",
    "word1_august_context['cleanspeech'] = word1_august_context['speech'].str.replace('[^\\w\\s]','').str.lower() # remove punctuation, lowercase\n",
    "word1_august_context['stopworded'] = word1_august_context['cleanspeech'].str.replace(stopwords_regex, '') #stopwording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the other words that appear in the context of word1 in 1967\n",
    "word1_may_context_count = word1_may_context[\"stopworded\"].str.split().explode().dropna().value_counts()\n",
    "word1_may_context_count[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the other words that appear in the context of word1 in 1967\n",
    "word1_august_context_count = word1_august_context[\"stopworded\"].str.split().explode().dropna().value_counts()\n",
    "word1_august_context_count[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data for word1 at different moments in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times was word1 was used in individual speeches in particular months? We just counted.  Let's call up the results and look.\n",
    "\n",
    "Note that in the table below there are two columns: a left-hand column which is an \"index;\" those are just the finding-aid numbers of the speeches.  A right-hand column gives the number of times word1 appears in that speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1_may_speechnumbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the speech that mentions word1 the most -- the speech listed on the top.  Find its index number in the left-hand column. \n",
    "\n",
    "We can look up information about that speech -- for instance the speaker and the text of the speech -- using its index number.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who gave the speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1_may_context['speaker'][58751][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did the speech say?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1_may_context['speech'][58751][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've cut the quote short.  You can see the whole speech for yourself by deleting the [:1000] at the end of the line of code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The importance of keeping going\n",
    "\n",
    "Keep going until you understand! You'll want to quote the evidence you gather.  Note that these speeches are selected at random from the list of speechnumbers that appear in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1_may_context['speech'][55468]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use command-F or EDIT > FIND in the menu above to search for word1 in your screen until you see the context.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data for word1 in August"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at word1 in August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1_august_speechnumbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw above that the word \"malpractice\" was invoked far more frequently in 2010 than in 1967.  Here, we see that the speeches that invoked 'malpractice' in 2010 also invoke that term more frequently -- up to 7 times in one speech -- than the one or two times each speech used the term in 1967.  \n",
    "\n",
    "Such a shift suggests that the word 'malpractice' is being applied with a more specific meaning in 2010. But to interpret this shift, we need to read more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the top speech, from row 7016.  Who gave the speech?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1_august_context['speaker'][119439]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did the speech say?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1_august_context['speech'][119439]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the same search with new vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you research shifts in words, typically you won't work on a single word at a time. You'll begin with a controlled vocabulary of the kind we used to investigate discussions of crime in Congress last week. Then you'll narrow down to a few words whose meaning you need to understand better.\n",
    "\n",
    "We can use a 'for' loop to cycle through a dozen words very quickly, producing an analysis.  \n",
    "\n",
    "We can even add a few lines to this for loop to ask the computer to print out all the relevant speeches.  I have abbreviated the number of words listed and the lines of the speeech given for room.\n",
    "\n",
    "When you do your assignment, begin by zeroing in on certain words in the list of keywords on the first line.  \n",
    "\n",
    "Next, tweak the code to show more words and lines of the speech when you get to your own research.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_laws[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word1 in top_laws['law'][:10]:\n",
    "    \n",
    "  # get the rownumbers of the n speeches that use word1 the most\n",
    "    word1_may_speechnumbers = may_1968['speech'].str.count(word1).nlargest(2)  ########### <-- play with this number *********\n",
    "    word1_august_speechnumbers = august_1968['speech'].str.count(word1).nlargest(2)  ########### <-- play with this number *********\n",
    "\n",
    "    # get the speeches that mentioned word1 the most\n",
    "    word1_may_context = may_1968.loc[list(word1_may_speechnumbers.index)]\n",
    "    word1_august_context = august_1968.loc[list(word1_august_speechnumbers.index)]\n",
    "\n",
    "    # clean up the data\n",
    "    word1_may_context['speech'] = word1_may_context['speech'].str.replace('[^\\w\\s]','').str.lower() # remove punctuation, lowercase\n",
    "    word1_may_context['stopworded'] = word1_may_context['speech'].str.replace(stopwords_regex, '') #stopwording\n",
    "\n",
    "    word1_august_context['speech'] = word1_august_context['speech'].str.replace('[^\\w\\s]','').str.lower() # remove punctuation, lowercase\n",
    "    word1_august_context['stopworded'] = word1_august_context['speech'].str.replace(stopwords_regex, '') #stopwording\n",
    "\n",
    "    # print some context for May\n",
    "    print(\"*****\")\n",
    "    print(\"CONTEXT for \"+word1+\" in May 1968\")\n",
    "    for speechnumber in list(word1_may_speechnumbers.index)[:2]:  ########### <-- play with this number *********\n",
    "        speaker = may_1968['speaker'][speechnumber]\n",
    "        date = may_1968['date'][speechnumber]\n",
    "        print('speech by '+ speaker + ' in ' + date)\n",
    "        print(may_1968['speech'][speechnumber][:100])  ########### <-- play with this number *********\n",
    "\n",
    "    # count the other words that appear in the context of word1 in May\n",
    "    word1_may_context_count = word1_may_context[\"stopworded\"].str.split().explode().dropna().value_counts()\n",
    "    print(\"***\")\n",
    "    print(\"COLLOCATES for \" + word1 + \" in May 1968\")\n",
    "    print(word1_may_context_count[:5])  # <-- play with this number\n",
    "\n",
    "    # print some context for August\n",
    "    print(\"*****\")\n",
    "    print(\"CONTEXT for \"+ word1+\" in August 1968\")\n",
    "    for speechnumber in list(word1_august_speechnumbers.index)[:2]:  ########### <-- play with this number *********\n",
    "        speaker = august_1968['speaker'][speechnumber]\n",
    "        date = august_1968['date'][speechnumber]\n",
    "        print('speech by '+ speaker+ ' in ' + date)\n",
    "        print(august_1968['speech'][speechnumber][:100])  ########### <-- play with this number *********\n",
    "\n",
    "    # count the other words that appear in the context of word1 in 2010\n",
    "    word1_august_context_count = word1_august_context[\"stopworded\"].str.split().explode().dropna().value_counts()\n",
    "    print(\"COLLOCATES for \" + word1 + \" in August 1968\")\n",
    "    print(word1_august_context_count[:5])   ########### <-- play with this number *********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing KeyWords in Context (KWIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this last exercise we will find keywords in context (KWIC). You can think of KWIC's output like the lines of a concordance, where you look up a passage and find the context.  We saw in the reading that historian Luke Blaxill used KWIC to make arguments about how different words were used in British election speeches.\n",
    "\n",
    "KWIC is not the only view you need to understand the context of speeches, but it is a useful one.\n",
    "\n",
    "To produce this output, we will split the text into n-grams where \"n\" stands for any number. For example, a text split into single words (like our data frame in the previous example) is split into 1-grams (also just called \"tokens\"). 3-grams split the text into units that are 3 words long. 5-grams split the text into units that are 5 words long. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function that returns our keyword in context. You don't need to follow the code, but you should know that you can always use the function below when you want to generate a KWIC view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'Constitution'\n",
    "\n",
    "keyword_df = may_1968[may_1968['speech'].str.contains(keyword)].copy() # search the text for the presence of our keyword \n",
    "\n",
    "def KWIC(body):\n",
    "    n = 5 # specify the number of surrounding words to use before and after the keyword\n",
    "    \n",
    "    words = body.split() # split the words into tokens\n",
    "    keyword_index = 0\n",
    "    \n",
    "    # The following for loop iterates through each word in the text. If our keyword is found, the for loop stores its index. \n",
    "    # But, why are we storing the index? As you might remember, the index is a numerical representation of the row's position.\n",
    "    # It can also be thought of as a unique name the computer uses to identify a row. We can call the index to return JUST these rows.\n",
    "    for index, word in enumerate(words): \n",
    "        if keyword in word: \n",
    "            keyword_index = index\n",
    "            break\n",
    "    \n",
    "    before_keyword = words[max(0, keyword_index - n):keyword_index] # store the words that come before the keyword, up to our specified number \n",
    "    after_keyword = words[keyword_index:keyword_index + n] # store the words that come after the keyword, up to our specified number \n",
    "    return ' '.join(before_keyword + after_keyword) # return the keyword in its context\n",
    "\n",
    "keyword_df['context'] = keyword_df['speech'].apply(KWIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df[['context']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data frame is a little difficult to look at, so I am going to export the contents to a file.  You can find this file in the left-hand sidebar of Jupyter and open it on your home computer to inspect later via an application such as excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df['context'].to_csv('KWIC_example_keyword_constitution_may_1968.csv', sep = ' ', index=False, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's return to extracting named entities. Let's make it faster and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd probably like to search for multipe categories.  \n",
    "\n",
    "Let's create a function that will apply named entity recognition for a single category, count the stuff over months, find the stuff mentioned over multiple months, and plot it in a line chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just run the named entity recognizer one more time over our data to get a list of all possible named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***This may take some time. Give it an hour or downsample.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the basic code to extract named entities from each speech for Congress in the year 1968.  \n",
    "\n",
    "    named_entities1 = [nlp(doc) for doc in congress_1968['speech']]\n",
    "    \n",
    "-- or with .apply:\n",
    "\n",
    "    named_entities1 = congress_1968['speech'].apply(lambda x: nlp(x))\n",
    "    \n",
    "Both will get you to the same place. \n",
    "\n",
    "Below, we add some fancy new functions to (1) parallelize for speed and (2) add a progress report that will tell us how far we are through the corpus.  You do not need to know how the code works; just enjoy the power!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Over All of 1968 with a Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logged_apply(g, func, *args, **kwargs):\n",
    "    step_percentage = 100. / len(g)\n",
    "    import sys\n",
    "    sys.stdout.write('apply progress:   0%')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    def logging_decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            progress = wrapper.count * step_percentage\n",
    "            progress1 = round(progress, 3)\n",
    "            if (progress *100000) % 10000 == 0:  \n",
    "                sys.stdout.write('\\033[D \\033[D' * 4 + format(progress, '3.0f') + '%')\n",
    "            sys.stdout.flush()\n",
    "            wrapper.count += 1\n",
    "            return func(*args, **kwargs)\n",
    "        wrapper.count = 0\n",
    "        return wrapper\n",
    "\n",
    "    logged_func = logging_decorator(func)\n",
    "    res = g.apply(logged_func, *args, **kwargs)\n",
    "    sys.stdout.write('\\033[D \\033[D' * 4 + format(100., '3.0f') + '%' + '\\n')\n",
    "    sys.stdout.flush()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities1 = logged_apply(congress_1968['speech'], lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing in Parallel With Multiple Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import marshal\n",
    "\n",
    "n = multiprocessing.cpu_count()\n",
    "\n",
    "def parallelize_operation(df, func, n_cores = n):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def lambda_nlp(df): \n",
    "    out = df.apply(nlp)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities1 = parallelize_operation(congress_1968['speech'], lambda_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Function to Do Our Counting and Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function, ner_mapper.\n",
    "\n",
    "The point of our creating a function here is to show that we can recapitulate all of the code in this notebook, starting with the original dataframe, in such a way that we can efficiently mine the text for multiple kinds of named entity (ORG, GPE, PERSON, etc), thus allowing us to make multiple graphs.\n",
    "\n",
    "Following the line beginning with \"def\", all of the code *inside* the function is code that you have seen before.  \n",
    "\n",
    "Here is what happens.\n",
    "\n",
    "In the line beginning with \"def,\" we define a new function, naming it, ner_mapper.  We also tell Python that our new function will take three objects:\n",
    "\n",
    "   * dataframe1 - a dataframe with column \"Month-Year\"\n",
    "   * column1 - a parsed column of named entities, corresponding to the \"text\" column of dataframe1\n",
    "   * label1 - a label of Spacy named entity categories, for example 'GPE' or 'ORG'\n",
    "   \n",
    "Inside the function:\n",
    "\n",
    "   * We filter just for the named entities that match label1.\n",
    "   * We group those named entities by month-year and count how many there are.\n",
    "   * We find the named entities that are spoken about over 3 months or more.\n",
    "   * We plot a line graph showing those named entities over time.\n",
    "   * We save the figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "palette = plt.get_cmap('tab20b') # this tells matplotlib what colors to use.  you can play with this to change the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_mapper(dataframe1, column1, label1):\n",
    "    \n",
    "    print('*****************')\n",
    "    print('LOOKING FOR ', label1)\n",
    "    print('**')\n",
    "    \n",
    "    # filter only for the named entities matching label1 and convert Spacy data to a list of strings\n",
    "    new_column = [[str(ent) for ent in doc.ents if ent.label_ == label1] for doc in column1]\n",
    "    \n",
    "    # add named entities back to dataframe, explode data\n",
    "    dataframe1['stuff'] = new_column    \n",
    "    dataframe1 = dataframe1.explode('stuff').dropna()\n",
    "    \n",
    "    # a cleaning step\n",
    "    stopwords = ['Chair', 'Speaker', 'Congress', 'House', 'Senate', 'State', 'Chairman']\n",
    "    dataframe2 = [row for row in dataframe1 if dataframe1['stuff'] not in stopwords]\n",
    "    dataframe2['stuff'] = dataframe2['stuff'].str.replace('the United States', 'U.S.')\n",
    "    dataframe2['stuff'] = dataframe2['stuff'].str.title() # make punctuation consistent\n",
    "    dataframe2['stuff'] = dataframe2['stuff'].str.replace('[^\\w\\s]','') # remove punctuation\n",
    "    \n",
    "    # count entities by month_year\n",
    "    stuff_per_month = dataframe2.groupby(['month_year', 'stuff'])['stuff'].count()\n",
    "    stuff_per_month = pd.DataFrame(stuff_per_month) # convert from Series to DataFrame\n",
    "    stuff_per_month.columns = ['count'] # label the one column \"count\"\n",
    "    stuff_per_month = stuff_per_month.reset_index() # turn the multi-index into columns 'year' and 'vocab'\n",
    "    stuff_per_month[:5]\n",
    "\n",
    "    # find max per month\n",
    "    top_stuff_per_month = stuff_per_month.groupby(['month_year']).apply(lambda x: x.sort_values('count', ascending = False)).reset_index(drop=True)\n",
    "    top_stuff_per_month = top_stuff_per_month.groupby('month_year').head(3)\n",
    "    top_stuff_per_month[:20]      \n",
    "\n",
    "    top_over_time = stuff_per_month[stuff_per_month['stuff'].isin(top_stuff_per_month['stuff'].tolist() + top_stuff['stuff'].tolist())] # get the occurrences per month of the stuff talked about in multiple monhts\n",
    "    \n",
    "    # format time\n",
    "    top_over_time.loc[:,'month_year'] = pd.to_datetime(top_over_time['month_year'])\n",
    "\n",
    "    # Graph entities \n",
    "    plt.clf() # clear last output\n",
    "    plt.figure(figsize=(8, 6), dpi=300)\n",
    "    \n",
    "    plt.style.use('seaborn-darkgrid') # this gives us a grid with a dark background.  you can play with this to change the style.\n",
    "    num=0\n",
    "\n",
    "    for entity in set(top_over_time['stuff']): # one loop for each color/line\n",
    "        \n",
    "        # get points\n",
    "        num+=1 # for each new word, the counter 'num' changes \n",
    "        x = top_over_time.loc[top_over_time['stuff'] == entity, 'month_year'] # x points\n",
    "        y = top_over_time.loc[top_over_time['stuff'] == entity, 'count'] # y points\n",
    "        x2, y2 = zip(*sorted(zip(x, y))) # get everything in the right order\n",
    "        \n",
    "        # make a line\n",
    "        plt.plot(x2, # x axis \n",
    "             y2,  # y axis\n",
    "             '-o', # make dots with lines\n",
    "             color=palette(num), alpha=0.7, label=entity) # num tells the plot to choose a different color this time\n",
    "        \n",
    "        # make labels\n",
    "        y3 = max(y2) # label lines at their highest point\n",
    "        entity_points = top_over_time[top_over_time['stuff'] == entity]\n",
    "        x3 = random.choice(entity_points[entity_points['count'] == max(entity_points['count'])]['month_year'].tolist()) # more for finding the highest point\n",
    "        plt.text(x3, y3, entity, color = palette(num), size = 10) # this is the code to supply a label for each line\n",
    "\n",
    "    legd = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5)) # move the legend\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Figure: Top \" + label1 + \"s Mentioned in Congress, 1968\", loc='left', fontsize=25, fontweight=5, color='Blue')\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.subplots_adjust(right=0.9) # add padding to the right so it doesn't get cut off\n",
    "    plt.savefig('congress-1968-ner-for-' + label1 + '-.jpg', bbox_inches='tight') # save - with extra commands so it doesn't get cut off \n",
    "    plt.figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out our new function. It should run smoothly because we've already gathered the time-intensive NER information in a separate dataframe, named_entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner_mapper(congress_1968, named_entities1, 'EVENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a for loop to cycle through multiple categories.  ***This will also take time. You're welcome to skip it. Just notice that we can compare lots of analyses if we reduce our complex cold to a function and loop through multiple aspects of the data.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cat in ['LOC', 'PERSON', 'GPE', 'EVENT', 'ORG']:\n",
    "#    ner_mapper(congress_1968, named_entities1, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do all the minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're probably already hating the process of waiting for Spacy to run nlp() to parse the named entities.  That's right -- it's time consuming and slow, and that's for one of our smallest datasets. \n",
    "\n",
    "If or when you decide to run Spacy's NER on your own dataset, you'll probably have to deal with a lot of waiting. However, you can follow some of the processes in this notebook to help you along:\n",
    "\n",
    "  * You can try different modes of doing the same analysis. You can use timing code to keep track of which process is fastest.\n",
    "  * You can write functions that work on a small subset of data. \n",
    "  * Once you have a working prototype for your code, you can apply your functions to a larger set of data and leave the computer running all night -- or all week or month if you have to.  \n",
    "\n",
    "If you're running code for a full day or even week, there are extra options that will allow you to start your Python code and walk away, downloading the results when everything is finished. Check in with the M2 staff if you need more guidance about running slow Python code.\n",
    "\n",
    "For THIS data, however, we've already run the boring part for you!  We ran the Spacy parser on the full dataset overnight, and stored the results in the  folder below.  The new function that follows, ner_mapper_congress, reads in the stored datasets for each possible NER label.  mYou just have to run the function, loading saved information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Here's the code to extract the named entities from the entire set of congress 1967-2010:\n",
    "#named_entities = [nlp(doc) for doc in congress['speech']] # without progress bar \n",
    "#named_entities = logged_apply(congress['speech'], lambda x: nlp(x)) # with progress bar\n",
    "named_entities = parallelize_operation(congress_1968['speech'], lambda_nlp) # over multiple cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### save each entity as a csv for later use\n",
    "for cat in ['LAW', 'LOC', 'PERSON', 'GPE', 'EVENT', 'ORG', 'LANGUAGE']:\n",
    "        \n",
    "    print('*****************')\n",
    "    print('LOOKING FOR ', cat)\n",
    "    print('**')\n",
    "    \n",
    "    # filter only for the named entities matching label1 and convert Spacy data to a list of strings\n",
    "    new_column = [[str(ent) for ent in doc.ents if ent.label_ == cat] for doc in named_entities]\n",
    "    \n",
    "    # count overall instances of named entities\n",
    "    stuffcount = []\n",
    "    for row in new_column:\n",
    "        for item in row:\n",
    "            stuffcount.append(str(item))\n",
    "    stuffcount[:5]\n",
    "    top_stuff = pd.Series.value_counts(stuffcount).nlargest(10)\n",
    "    top_stuff = pd.DataFrame(top_stuff).reset_index()\n",
    "    top_stuff.columns = ['stuff', 'count']\n",
    "    top_stuff = top_stuff[top_stuff['count'] > 1]\n",
    "\n",
    "    print(\"The top instances of \" + cat + \" are:\")\n",
    "    print(top_stuff[:5])\n",
    "\n",
    "    # add named entities back to dataframe, explode data, count entities by Month-Year\n",
    "    dataframe1['stuff'] = new_column\n",
    "    dataframe1 = dataframe1.explode('stuff').dropna()\n",
    "    stuff_per_month = dataframe1.groupby(['month_year', 'stuff'])['stuff'].count()\n",
    "    stuff_per_month = pd.DataFrame(stuff_per_month) # convert from Series to DataFrame\n",
    "    stuff_per_month.columns = ['count'] # label the one column \"count\"\n",
    "    stuff_per_month = stuff_per_month.reset_index() # turn the multi-index into columns 'year' and 'vocab'\n",
    "    stuff_per_month.to_csv('named-entities-per-month-for-congress-1967-2010-' + cat + \".csv\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_mapper_all_congress(label1):\n",
    "\n",
    "    stuff_per_month = pd.read_csv('named-entities-per-month-for-congress-1967-2010-' + label1 + \".csv\") # save a copy of the data\n",
    "\n",
    "    # find max per month\n",
    "    top_stuff_per_month = stuff_per_month.groupby(['month_year']).apply(lambda x: x.sort_values('count', ascending = False)).reset_index(drop=True)\n",
    "    top_stuff_per_month = top_stuff_per_month.groupby('month_year').head(3)\n",
    "    top_stuff_per_month[:20]      \n",
    "\n",
    "    top_over_time = stuff_per_month[stuff_per_month['stuff'].isin(top_stuff_per_month['stuff'].tolist() + top_stuff['stuff'].tolist())] # get the occurrences per month of the stuff talked about in multiple monhts\n",
    "    \n",
    "    # format time\n",
    "    top_over_time.loc[:,'month_year'] = pd.to_datetime(top_over_time['month_year'])\n",
    "\n",
    "    # Graph entities \n",
    "    plt.clf() # clear last output\n",
    "    plt.figure(figsize=(8, 6), dpi=300)\n",
    "    \n",
    "    plt.style.use('seaborn-darkgrid') # this gives us a grid with a dark background.  you can play with this to change the style.\n",
    "    num=0\n",
    "\n",
    "    for entity in set(top_over_time['stuff']): # one loop for each color/line\n",
    "        \n",
    "        # get points\n",
    "        num+=1 # for each new word, the counter 'num' changes \n",
    "        x = top_over_time.loc[top_over_time['stuff'] == entity, 'month_year'] # x points\n",
    "        y = top_over_time.loc[top_over_time['stuff'] == entity, 'count'] # y points\n",
    "        x2, y2 = zip(*sorted(zip(x, y))) # get everything in the right order\n",
    "        \n",
    "        # make a line\n",
    "        plt.plot(x2, # x axis \n",
    "             y2,  # y axis\n",
    "             '-o', # make dots with lines\n",
    "             color=palette(num), alpha=0.7, label=entity) # num tells the plot to choose a different color this time\n",
    "        \n",
    "        # make labels\n",
    "        y3 = max(y2) # label lines at their highest point\n",
    "        entity_points = top_over_time[top_over_time['stuff'] == entity]\n",
    "        x3 = random.choice(entity_points[entity_points['count'] == max(entity_points['count'])]['month_year'].tolist()) # more for finding the highest point\n",
    "        plt.text(x3, y3, entity, color = palette(num), size = 10) # this is the code to supply a label for each line\n",
    "\n",
    "    legd = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5)) # move the legend\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Figure: Top \" + label1 + \"s Mentioned in Congress, 1967-2010\", loc='left', fontsize=25, fontweight=5, color='Blue')\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.subplots_adjust(right=0.9) # add padding to the right so it doesn't get cut off\n",
    "    plt.savefig('congress-1968-ner-for-' + label1 + '-.jpg', bbox_inches='tight') # save - with extra commands so it doesn't get cut off \n",
    "    plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in ['LAW', 'LOC', 'PERSON', 'GPE', 'EVENT', 'ORG', 'LANGUAGE']:\n",
    "    ner_mapper_all_congress(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
